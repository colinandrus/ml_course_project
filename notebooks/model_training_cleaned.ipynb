{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, log_loss, \\\n",
    "    precision_score, recall_score, precision_recall_curve, average_precision_score, \\\n",
    "    confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline, Pipeline \n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pdpbox import pdp\n",
    "from sklearn.calibration import calibration_curve \n",
    "from sklearn.base import clone \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder where data resides \n",
    "DATAFOLDER = \"~/Documents/data-science-coursework/nyu-ml/project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 242466 entries, 4657002.0 to 5021568.0\n",
      "Data columns (total 43 columns):\n",
      "idncase                                242466 non-null int64\n",
      "idnproceeding                          242466 non-null int64\n",
      "nat_grouped                            242466 non-null object\n",
      "lang_grouped                           242466 non-null object\n",
      "ij_code_grouped                        242466 non-null object\n",
      "Male_judge                             226481 non-null float64\n",
      "Year_Appointed_SLR                     226481 non-null float64\n",
      "Year_College_SLR                       226481 non-null float64\n",
      "Year_Law_school_SLR                    226481 non-null float64\n",
      "Government_Years_SLR                   226481 non-null float64\n",
      "Govt_nonINS_SLR                        226481 non-null float64\n",
      "INS_Years_SLR                          226481 non-null float64\n",
      "Military_Years_SLR                     226481 non-null float64\n",
      "NGO_Years_SLR                          226481 non-null float64\n",
      "Privateprac_Years_SLR                  226481 non-null float64\n",
      "Academia_Years_SLR                     226481 non-null float64\n",
      "judge_missing_bio                      242466 non-null int64\n",
      "years_since_judge_appointment          242466 non-null float64\n",
      "years_since_law_school                 242466 non-null float64\n",
      "last_10_appeal_grant_by_judge          235220 non-null float64\n",
      "last_10_appeal_grant_by_judge_nat      196313 non-null float64\n",
      "lawyer                                 242466 non-null int64\n",
      "defensive                              236930 non-null float64\n",
      "affirmative                            236930 non-null float64\n",
      "oral                                   242026 non-null float64\n",
      "written                                242026 non-null float64\n",
      "case_type_string                       242466 non-null object\n",
      "original_dec_string                    242435 non-null object\n",
      "strCustody                             202453 non-null object\n",
      "strProbono                             476 non-null object\n",
      "base_city_code                         242466 non-null object\n",
      "hearing_loc_match_base                 242466 non-null object\n",
      "datAppealFiled_year                    242466 non-null float64\n",
      "datAppealFiled_year_month              242466 non-null float64\n",
      "comp_year                              242466 non-null int64\n",
      "comp_year_month                        242466 non-null int64\n",
      "comp_days_elasped_since_input_date     242466 non-null float64\n",
      "input_days_elapsed_since_osc_date      242466 non-null float64\n",
      "appeal_days_elapsed_since_comp_date    242466 non-null int64\n",
      "datBIADecision_dt                      242466 non-null object\n",
      "datAppealFiled_dt                      242466 non-null object\n",
      "comp_dt                                242466 non-null object\n",
      "granted                                242466 non-null int64\n",
      "dtypes: float64(23), int64(8), object(12)\n",
      "memory usage: 81.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "df = pd.read_csv(os.path.join(DATAFOLDER, \"data_for_model/appeals_data_final.csv\"))\n",
    "df.set_index('idnAppeal', inplace=True) \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data\n",
    "\n",
    "def impute_columns_udf(col, imputation_type): \n",
    "    \"\"\" \n",
    "    UDF to specify imputation method for a given variable. \n",
    "    col is the pd.Series for which you want to impute, imputation_type should be chosen from \n",
    "    ['mode', 'mean', 'median', 'zero', 'none'] \n",
    "    \"\"\" \n",
    "    if imputation_type == 'mode':\n",
    "        val = col.mode()[0]\n",
    "        return col.fillna(val)\n",
    "    \n",
    "    elif imputation_type == 'mean': \n",
    "        val = col.mean() \n",
    "        return col.fillna(val)\n",
    "    \n",
    "    elif imputation_type == 'median': \n",
    "        val = col.median() \n",
    "        return col.fillna(val)\n",
    "    \n",
    "    elif imputation_type == 'zero': \n",
    "        return col.fillna(0)\n",
    "    \n",
    "    elif imputation_type == 'none':\n",
    "        return col.fillna('None')\n",
    "    \n",
    "    else: \n",
    "        raise ValueError('imputation_type argument not valid')\n",
    "\n",
    "class ImputeMissingData(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Pipeline step that impute missing values, given impute methods specified. \n",
    "        Note that numerical features with NaN's and no impute methods will raise an error, \n",
    "        categorical features on the other hand will be filled with 'None' by default unless specified otherwise.\n",
    "    \"\"\"\n",
    "    def __init__(self, impute_methods, num_features, cat_features): \n",
    "        self.impute_methods = impute_methods\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "    \n",
    "    def fit(self, X, y=None): \n",
    "        return self \n",
    "    \n",
    "    def transform(self, input_data): \n",
    "        output_data = input_data.copy() \n",
    "\n",
    "        # check which features have missing values but imputation methods not specified \n",
    "        features_nulls = output_data.isnull().sum() \n",
    "        features_nulls = features_nulls[features_nulls > 0].index.tolist() \n",
    "        features_need_impute = [f for f in features_nulls if f not in self.impute_methods.keys()]\n",
    "        num_features_need_impute = [f for f in features_need_impute if f in self.num_features]\n",
    "        cat_features_need_impute = [f for f in features_need_impute if f in self.cat_features]\n",
    "        \n",
    "        # raise exception for numerical features with missing values and no imputation method specified \n",
    "        if num_features_need_impute: \n",
    "            raise Exception(\"\"\"These numerical features have missing values: {}. \n",
    "            Please specify their impute methods.\"\"\".format(num_features_need_impute)) \n",
    "        \n",
    "        # set imputation method as 'none' for cat features with missing values with no imputation method specified \n",
    "        if cat_features_need_impute: \n",
    "            for cat_f in cat_features_need_impute: \n",
    "                self.impute_methods[cat_f] = 'none'\n",
    "            print(\"\"\"{} have missing values with no imputation method specified. \n",
    "            By default, they have been filled with 'None'.\"\"\".format(cat_features_need_impute)) \n",
    "                \n",
    "        # apply imputations \n",
    "        for col, imp_method in self.impute_methods.items(): \n",
    "            output_data[col] = impute_columns_udf(output_data[col], imp_method)\n",
    "            \n",
    "        return output_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data types\n",
    "class ConvertDataTypes(BaseEstimator, TransformerMixin): \n",
    "    \"\"\" Coerce data types to specifications defined by CAT_FEATURES and NUM_FEATURES \"\"\"\n",
    "    def __init__(self, num_features, cat_features): \n",
    "        self.num_features = num_features \n",
    "        self.cat_features = cat_features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, input_data): \n",
    "        output_data = input_data.copy() \n",
    "        output_data[self.num_features] = output_data[self.num_features].astype(float)\n",
    "        output_data[self.cat_features] = output_data[self.cat_features].astype(str)\n",
    "        return output_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummify data \n",
    "\n",
    "class Dummify(BaseEstimator, TransformerMixin): \n",
    "    \"\"\" Pipeline step that dummifies all categorical variables \"\"\"\n",
    "    def __init__(self, cat_feature_values): \n",
    "        self.cat_feature_values = cat_feature_values\n",
    "    \n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    \n",
    "    def transform(self, input_data):\n",
    "        output_data = input_data.copy()\n",
    "        # specify values categorical features can take on, to ensure train/test DF have same cols \n",
    "        cat_features = self.cat_feature_values.keys()\n",
    "        for col in cat_features: \n",
    "            output_data[col] = pd.Categorical(output_data[col], categories=self.cat_feature_values[col])        \n",
    "        output_data = pd.get_dummies(output_data, columns=cat_features, prefix_sep=':::')\n",
    "        return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to subset and transform data (impute, convert, dummify)\n",
    "\n",
    "def transform_features(df, appeals_df, cat_features, num_features, impute_methods): \n",
    "    \"\"\" Impute, convert and dummify features \n",
    "        TODO: save cat_feature_values to pickle so that we don't have to read df \n",
    "    \"\"\"  \n",
    "    \n",
    "    # makes sure dummified values match \n",
    "    cat_feature_values = dict([(f, [str(x) for x in appeals_df[f].dropna().unique().tolist()]) for f in cat_features])\n",
    "    \n",
    "    # make pipeline \n",
    "    data_pipeline = Pipeline([\n",
    "        ('impute', ImputeMissingData(impute_methods, num_features, cat_features)),         \n",
    "        ('convert_dtypes', ConvertDataTypes(num_features, cat_features)), \n",
    "        ('dummify', Dummify(cat_feature_values))\n",
    "    ])\n",
    "    \n",
    "    # run pipeline \n",
    "    X = data_pipeline.fit_transform(df)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# method to split train-test and transform features \n",
    "\n",
    "def get_model_data(df, appeals_df, label, cat_features, num_features, impute_methods, test_size=0.2, print_summary=False): \n",
    "    \"\"\" Subsets features used, splits into train-test, and transforms features \"\"\"\n",
    "    \n",
    "    # subsets features we are interested in \n",
    "    data = df[cat_features + num_features + [label]].copy() \n",
    "    appeals_df = appeals_df[cat_features + num_features].copy() \n",
    "    \n",
    "    # train test split \n",
    "    X, y = data.drop(label, axis=1).copy(), data[label].copy() \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=44)\n",
    "\n",
    "    # transform features\n",
    "    X_train = transform_features(x_train, appeals_df, cat_features, num_features, impute_methods)\n",
    "    X_test = transform_features(x_test, appeals_df, cat_features, num_features, impute_methods)\n",
    "\n",
    "    if print_summary:\n",
    "        print(\"Training Data: {} | Test Data: {}\".format(X_train.shape, X_test.shape)) \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to return model evaluation metrics \n",
    "def evaluate_model(truth, pred, print_cm=False): \n",
    "    \"\"\" Takes in arrays of truth and pred y values and return accuracy, logloss, roc_auc, and plot ROC \"\"\" \n",
    "    accuracy = accuracy_score(truth, (pred>0.5).astype(int))\n",
    "    logloss = log_loss(truth, pred)\n",
    "    fpr, tpr, thresholds = roc_curve(truth, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    precision = precision_score(truth, (pred>0.5).astype(int))\n",
    "    recall = recall_score(truth, (pred>0.5).astype(int))\n",
    "    if print_cm: \n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(truth, (pred>0.5).astype(int)))\n",
    "    metrics = {'Accuracy': accuracy, 'ROC AUC': roc_auc, 'Log Loss': logloss, \n",
    "               'Precision': precision, 'Recall': recall}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to plot ROC \n",
    "\n",
    "def plot_roc(truth, pred, model_name=None, title=None): \n",
    "    \"\"\" Takes in arrays of truth classes and pred probs to plot ROC curve \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(truth, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if model_name is not None: \n",
    "        plt.plot(fpr, tpr, label= '{0} (AUC = {1:.3f})'.format(model_name, roc_auc)) \n",
    "    else: \n",
    "        plt.plot(fpr, tpr, label= 'AUC:{0:.3f}'.format(roc_auc)) \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    if title is not None: \n",
    "        plt.title(title)\n",
    "    else: \n",
    "        plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to plot precision-recall curve \n",
    "\n",
    "def plot_precision_recall(truth, pred, model_name=None, title=None): \n",
    "    \"\"\" Takes in arrays of truth classes and pred probs to plot precision-recall curve\n",
    "        Code borrowed from http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "    \"\"\"    \n",
    "    precision, recall, _ = precision_recall_curve(truth, pred)\n",
    "    average_precision = average_precision_score(truth, pred)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b', \n",
    "                     label='Avg Precision:{0:.3f}'.format(average_precision))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    if title is not None: \n",
    "        plt.title(title)\n",
    "    else: \n",
    "        plt.title('Precision-Recall curve') \n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to train a model, output results, and plot AUC \n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, print_metrics=True, print_charts=False, \n",
    "                sample_weight=None, print_cm=False):\n",
    "    if sample_weight is not None: \n",
    "        model.fit(X_train, y_train.values.ravel(), sample_weight)\n",
    "    else: \n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "    truth = y_test.values.ravel()\n",
    "    pred = model.predict_proba(X_test)[:,1]\n",
    "    metrics = evaluate_model(truth, pred, print_cm)\n",
    "    try: \n",
    "        fi, cfi = get_feature_importances(model, X_train)\n",
    "    except AttributeError: \n",
    "        fi, cfi = None, None \n",
    "    \n",
    "    if print_metrics: \n",
    "        print(metrics)\n",
    "    if print_charts: \n",
    "        plot_metrics(truth, pred)\n",
    "    return model, metrics, fi, cfi, truth, pred   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to plot ROC and Precision-Recall\n",
    "\n",
    "def plot_metrics(truth, pred): \n",
    "    \"\"\" Plots ROC and Precision-Recall curves \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_roc(truth, pred)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_precision_recall(truth, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get feature importances \n",
    "\n",
    "def get_feature_importances(model, X_train):\n",
    "    \n",
    "    \"\"\" Takes model and train data as inputs, outputs regular and collapsed feature importances \"\"\"\n",
    "    \n",
    "    # get 'regular' feature importances \n",
    "    fi = pd.Series(data=model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "    \n",
    "    # get 'collapsed' feature importances (dummy variables of parent feature aggregated as one)\n",
    "    cfi = pd.DataFrame(fi).reset_index().rename(columns={'index': 'feature', 0: 'importance'})\n",
    "    cfi['parent_feature'] = cfi['feature'].apply(lambda x: x.split(':::')[0])\n",
    "    cfi = cfi.groupby('parent_feature')['importance'].sum().sort_values(ascending=False) \n",
    "    \n",
    "    return fi, cfi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 1: nationality only \n",
    "CAT_FEATURES = ['nat_grouped']\n",
    "NUM_FEATURES = []\n",
    "IMPUTE_METHODS = {'nat_grouped': 'none'}\n",
    "MODEL = (RandomForestClassifier(n_estimators=20, random_state=44))\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = CAT_FEATURES, \n",
    "                                                  num_features = NUM_FEATURES, impute_methods = IMPUTE_METHODS, \n",
    "                                                  print_summary=True) \n",
    "nat_model, nat_metrics, _, _, nat_truth, nat_pred  = train_model(MODEL, X_train, y_train, X_test, y_test, \n",
    "                                                  print_metrics=True, print_charts=True, print_cm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 2: judge only \n",
    "CAT_FEATURES = ['ij_code_grouped']\n",
    "NUM_FEATURES = []\n",
    "IMPUTE_METHODS = {}\n",
    "MODEL = (RandomForestClassifier(n_estimators=20, random_state=44))\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = CAT_FEATURES, \n",
    "                                                  num_features = NUM_FEATURES, impute_methods = IMPUTE_METHODS,\n",
    "                                                  print_summary=True) \n",
    "ij_model, ij_metrics, _, _, _, _ = train_model(MODEL, X_train, y_train, X_test, y_test, \n",
    "                                               print_metrics=True, print_charts=True, print_cm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 3: nat + judge \n",
    "CAT_FEATURES = ['nat_grouped', 'ij_code_grouped']\n",
    "NUM_FEATURES = []\n",
    "IMPUTE_METHODS = {'nat_grouped': 'none'}\n",
    "MODEL = (RandomForestClassifier(n_estimators=20, random_state=44))\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = CAT_FEATURES, \n",
    "                                                  num_features = NUM_FEATURES, impute_methods = IMPUTE_METHODS,\n",
    "                                                  print_summary=True) \n",
    "nat_ij_model, nat_ij_metrics, _, nat_ij_cfi, _, _ = train_model(MODEL, X_train, y_train, X_test, y_test, \n",
    "                                                                print_metrics=True, print_charts=True, print_cm=True)\n",
    "nat_ij_cfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 4: nat + judge + appeal year \n",
    "CAT_FEATURES = ['nat_grouped', 'ij_code_grouped']\n",
    "NUM_FEATURES = ['datAppealFiled_year']\n",
    "IMPUTE_METHODS = {'nat_grouped': 'none'}\n",
    "MODEL = (RandomForestClassifier(n_estimators=20, random_state=44))\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = CAT_FEATURES, \n",
    "                                                  num_features = NUM_FEATURES, impute_methods = IMPUTE_METHODS,\n",
    "                                                  print_summary=True) \n",
    "nat_ij_year_model, nat_ij_year_metrics, _, nat_ij_year_cfi, _, _ = train_model(MODEL, X_train, y_train, X_test, y_test,\n",
    "                                                                               print_metrics=True, print_charts=True, print_cm=True)\n",
    "nat_ij_year_cfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full model \n",
    "FULL_CAT_FEATURES = ['nat_grouped', 'ij_code_grouped', 'lang_grouped', 'case_type_string', 'Male_judge',\n",
    "                     'strCustody', 'strProbono', 'original_dec_string', 'base_city_code', 'hearing_loc_match_base']\n",
    "FULL_NUM_FEATURES = ['datAppealFiled_year', 'comp_year_month', 'lawyer', 'defensive', 'affirmative',\n",
    "                     'oral', 'written', 'comp_year', 'Year_Appointed_SLR', 'Year_College_SLR',\n",
    "                     'Year_Law_school_SLR', 'Government_Years_SLR', 'Govt_nonINS_SLR', 'INS_Years_SLR', \n",
    "                     'Military_Years_SLR', 'NGO_Years_SLR', 'Privateprac_Years_SLR', 'Academia_Years_SLR',\n",
    "                     'judge_missing_bio', 'appeal_days_elapsed_since_comp_date', 'comp_days_elasped_since_input_date', \n",
    "                     'input_days_elapsed_since_osc_date', 'years_since_judge_appointment', 'years_since_law_school', \n",
    "                     'last_10_appeal_grant_by_judge', 'last_10_appeal_grant_by_judge_nat'] \n",
    "\n",
    "FULL_IMPUTE_METHODS = {'nat_grouped': 'none', 'strCustody': 'none', 'strProbono': 'none', 'Male_judge': 'none', \n",
    "                       'original_dec_string': 'none', 'defensive': 'zero', 'affirmative': 'zero', 'oral': 'zero', \n",
    "                       'written': 'zero', 'Year_Appointed_SLR': 'median', 'Year_College_SLR': 'median',\n",
    "                       'Year_Law_school_SLR': 'median', 'Government_Years_SLR': 'median', \n",
    "                       'Govt_nonINS_SLR': 'median', 'INS_Years_SLR': 'median', 'Military_Years_SLR': 'median',\n",
    "                       'NGO_Years_SLR': 'median', 'Privateprac_Years_SLR': 'median', 'Academia_Years_SLR': 'median',\n",
    "                       'last_10_appeal_grant_by_judge': 'median', 'last_10_appeal_grant_by_judge_nat': 'median'} \n",
    "\n",
    "MODEL = (RandomForestClassifier(n_estimators=20, random_state=44))\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = FULL_CAT_FEATURES, \n",
    "                                                  num_features = FULL_NUM_FEATURES, \n",
    "                                                  impute_methods = FULL_IMPUTE_METHODS, print_summary=True) \n",
    "rf_model, rf_metrics, rf_fi, rf_cfi, rf_truth, rf_pred = train_model(MODEL, X_train, y_train, X_test, y_test, \n",
    "                                                                     print_metrics=True, print_charts=True, print_cm=True)\n",
    "rf_cfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_comparison = {'Nationality Only': nat_metrics, \n",
    "                       'Judge Only': ij_metrics, \n",
    "                       'Nat + Judge': nat_ij_metrics, \n",
    "                       'Nat + Judge + Appeal Year': nat_ij_year_metrics, \n",
    "                       'Full Model': rf_metrics}\n",
    "\n",
    "pd.DataFrame.from_dict(baseline_comparison, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Other Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = (GradientBoostingClassifier(loss='deviance', learning_rate=.1, n_estimators=20, random_state=44))\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = FULL_CAT_FEATURES, \n",
    "                                                  num_features = FULL_NUM_FEATURES, impute_methods = FULL_IMPUTE_METHODS)\n",
    "gb_model, gb_metrics, gb_fi, gb_cfi, gb_truth, gb_pred = train_model(MODEL, X_train, y_train, X_test, y_test, \n",
    "                                                                     print_metrics=True, print_charts=True, print_cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = (XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=20, seed=44))\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = FULL_CAT_FEATURES, \n",
    "                                                  num_features = FULL_NUM_FEATURES, impute_methods = FULL_IMPUTE_METHODS)\n",
    "xgb_model, xgb_metrics, xgb_fi, xgb_cfi, xgb_truth, xgb_pred = train_model(MODEL, X_train, y_train, X_test, y_test, \n",
    "                                                                           print_metrics=True, print_charts=True, print_cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = (LogisticRegression(penalty='l2', C=1))\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = FULL_CAT_FEATURES, \n",
    "                                                  num_features = FULL_NUM_FEATURES, impute_methods = FULL_IMPUTE_METHODS) \n",
    "lr_model, lr_metrics, lr_fi, lr_cfi, lr_truth, lr_pred = train_model(MODEL, X_train, y_train, X_test, y_test, \n",
    "                                                                     print_metrics=True, print_charts=True, print_cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Comparison (out-of-the-box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_comparison = {'Random Forest': rf_metrics, \n",
    "                   'Gradient Boosting': gb_metrics, \n",
    "                   'XGBoost': xgb_metrics, \n",
    "                   'Logistic Regression': lr_metrics}\n",
    "\n",
    "pd.DataFrame.from_dict(algo_comparison, orient='index').sort_values(by='ROC AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all ROC in a single chart \n",
    "plot_roc(rf_truth, rf_pred, 'Random Forest')\n",
    "plot_roc(lr_truth, lr_pred, 'Logistic Regression')\n",
    "plot_roc(gb_truth, gb_pred, 'Gradient Boosting')\n",
    "plot_roc(xgb_truth, xgb_pred, 'XGBoost')\n",
    "plt.title('Model Comparison by ROC AUC'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Random Forest outperforms other algorithms significantly in accuracy and ROC AUC out-of-the-box, we will move forward with Random Forest and tune it to get our best model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search scope \n",
    "max_features = ['sqrt']\n",
    "max_depth = list(np.arange(60, 140, 20))\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# construct param grid \n",
    "param_grid = {'max_features': max_features, 'max_depth': max_depth, \n",
    "              'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# run randomized search cv \n",
    "X_train, _, y_train, _ = get_model_data(df, df, label='granted', cat_features = FULL_CAT_FEATURES, \n",
    "                                        num_features = FULL_NUM_FEATURES, impute_methods = FULL_IMPUTE_METHODS) \n",
    "rf_clf = RandomForestClassifier(n_estimators=20, random_state=44)\n",
    "rf_gridsearch = GridSearchCV(rf_clf, param_grid, scoring=['roc_auc', 'accuracy', 'neg_log_loss'], refit='roc_auc')\n",
    "rf_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_cv(cv_object): \n",
    "    \"\"\" Print results for randomized search cv \"\"\"\n",
    "    rename_cols = {'param_max_depth': 'max_depth', \n",
    "                   'param_max_features': 'max_features', \n",
    "                   'param_min_samples_leaf': 'min_samples_leaf', \n",
    "                   'param_min_samples_split': 'min_samples_split', \n",
    "                   'mean_test_roc_auc': 'roc_auc', \n",
    "                   'mean_test_accuracy': 'accuracy', \n",
    "                   'mean_train_neg_log_loss': 'neg_log_loss'}\n",
    "    results = pd.DataFrame(cv_object.cv_results_)\n",
    "    results.rename(columns=rename_cols, inplace=True)\n",
    "    results = results[['max_depth', 'max_features', 'min_samples_leaf', 'min_samples_split', \n",
    "                       'roc_auc', 'accuracy', 'neg_log_loss']]\n",
    "    return results.sort_values(by='roc_auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_search_cv(rf_gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain with best params on 100 trees \n",
    "MODEL = (RandomForestClassifier(n_estimators=100, min_samples_split=2, max_features='sqrt', \n",
    "                                max_depth=60, min_samples_leaf=1, random_state=44))\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_model_data(df, df, label='granted', cat_features = FULL_CAT_FEATURES, \n",
    "                                                  num_features = FULL_NUM_FEATURES, impute_methods = FULL_IMPUTE_METHODS) \n",
    "rf_best_model, rf_best_metrics, rf_best_fi, rf_best_cfi, rf_best_truth, rf_best_pred = train_model(\n",
    "    MODEL, X_train, y_train, X_test, y_test, print_metrics=True, print_charts=True, print_cm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_comparison_tuned = {'Random Forest (Tuned + More Trees)': rf_best_metrics,\n",
    "                         'Random Forest': rf_metrics} \n",
    "\n",
    "pd.DataFrame.from_dict(algo_comparison_tuned, orient='index').sort_values(by='ROC AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature importances \n",
    "rf_best_fi.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check collapsed feature importances \n",
    "rf_best_cfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to pickle \n",
    "import pickle\n",
    "model_pkl_fname = \"aggregate_random_forest_vF.pkl\"\n",
    "#model_pkl_fname = os.path.join(DATAFOLDER, 'aggregate_random_forest_v0.pkl') # doesn't work? \n",
    "with open(model_pkl_fname, 'wb') as file:  \n",
    "    pickle.dump(rf_best_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability calibration can sometimes be an issue for random forest. But it appears that our classifier is already well-calibrated, so we deem it unnecessary to proceed with calibration methods like Platt Scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_outofbox = rf_model.predict_proba(X_test)[:,1]\n",
    "y_pred_best = rf_best_model.predict_proba(X_test)[:,1]\n",
    "calibration_outofbox = calibration_curve(y_test, y_pred_outofbox, n_bins=10)\n",
    "calibration_best = calibration_curve(y_test, y_pred_best, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(calibration_outofbox[0], calibration_outofbox[1], label='Random Forest')\n",
    "plt.plot(calibration_best[0], calibration_best[1], label='Random Forest (Tuned)')\n",
    "plt.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), label='Perfectly Calibrated', color='grey')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far we aggregated data across time as a single population from which we drew train and test samples from. This might not be the most realistic way to build a model that is used to predict future appeal outcomes (i.e. we do not have the luxury of using data from future appeals to inform the outcome of a current appeal). To more realistically assess the power of our predictive models, we will build a sequence of models: one model to predict each year's of appeals (by datAppealFiled_year) between 1991 and 2013 using data in preceeding years. In other words, we will only use data from 2000 and earlier to predict 2001's appeals, data from 2001 and earlier to predict 2002's appeals etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data_by_year(df, appeals_df, label, unique_id, cat_features, num_features, impute_methods, predict_year): \n",
    "    \"\"\" Generates features on data for years prior to predict_year \"\"\"\n",
    "\n",
    "    # add 'datAppealFiled_year' if not in num_features \n",
    "    if 'datAppealFiled_year' not in num_features: \n",
    "        num_features_ = num_features + ['datAppealFiled_year'] \n",
    "    else: \n",
    "        num_features_ = num_features \n",
    "\n",
    "    # subsets features we are interested in \n",
    "    cat_feature_values = dict([(f, [str(x) for x in df[f].dropna().unique().tolist()]) for f in cat_features]) \n",
    "    data = df.set_index(unique_id)\n",
    "    if label is not None: \n",
    "        data = data[cat_features + num_features_ + [label]].copy() \n",
    "    else: \n",
    "        data = data[cat_features + num_features].copy()\n",
    "\n",
    "    # train test split \n",
    "    train_data = data[data['datAppealFiled_year'] < predict_year]\n",
    "    test_data = data[data['datAppealFiled_year'] == predict_year]\n",
    "    \n",
    "    # return y=None if label is not passed (used for parsing data for pure predictions)\n",
    "    if label is not None: \n",
    "        x_train, y_train = train_data.drop(label, axis=1).copy(), train_data[label].copy() \n",
    "        x_test, y_test = test_data.drop(label, axis=1).copy(), test_data[label].copy() \n",
    "    else: \n",
    "        x_train, y_train = train_data, None \n",
    "        x_test, y_test = test_data, None \n",
    "\n",
    "    # transform \n",
    "    X_train = transform_features(x_train, appeals_df, cat_features, num_features, impute_methods)\n",
    "    X_test = transform_features(x_test, appeals_df, cat_features, num_features, impute_methods)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sequential_models(df, model, label, cat_features, num_features, impute_methods, \n",
    "                          start_year, end_year, weight_decay=None, print_charts=False, print_metrics=True): \n",
    "    \"\"\" Trains a sequence of models using data from preceeding years to test on current year \n",
    "        TODO: modularize this if have time\"\"\"\n",
    "    \n",
    "    # initialize empty dictionary to collect all results \n",
    "    sequential_results = {}\n",
    "    model_list = [] \n",
    "    \n",
    "    # loop through each year to train model on data from preceeding years then test on current year \n",
    "    for i, year in enumerate(np.arange(start_year, end_year+1, 1)): \n",
    "        print(\"Training model to predict {} appeals...\".format(year)) \n",
    "        result = {} # initialize empty dictionary to collect result for each year \n",
    "        model_list.append(clone(model)) # create a new copy of the model to train on new subset of data\n",
    "        X_train, X_test, y_train, y_test = get_model_data_by_year(\n",
    "            df, df, label='granted', unique_id='idnproceeding', cat_features=cat_features, num_features=num_features, \n",
    "            impute_methods=impute_methods, predict_year=year) \n",
    "        \n",
    "        # weight samples \n",
    "        if weight_decay is not None:\n",
    "            print(weight_decay)\n",
    "            sample_weight = X_train['datAppealFiled_year'].apply(lambda x: weight_decay ** (year-x-1))\n",
    "        else: \n",
    "            sample_weight = None \n",
    "        \n",
    "        # datAppealFiled_year is included in X_train by default; remove if not in num_features\n",
    "        if 'datAppealFiled_year' not in num_features: \n",
    "            X_train = X_train[[c for c in X_train.columns if c != 'datAppealFiled_year']]\n",
    "            X_test = X_test[[c for c in X_train.columns if c != 'datAppealFiled_year']]\n",
    "        else: \n",
    "            pass \n",
    "        \n",
    "        # save results to dictionary \n",
    "        result['model'], result['metrics'], result['fi'], result['cfi'], result['truth'], result['pred'] = train_model(\n",
    "            model_list[i], X_train, y_train, X_test, y_test, \n",
    "            print_charts=print_charts, print_metrics=print_metrics, sample_weight=sample_weight) \n",
    "        sequential_results[year] = result \n",
    "        \n",
    "    # summarize model performance metrics \n",
    "    metric_summary = pd.DataFrame.from_dict(sequential_results, orient='index')['metrics'].apply(pd.Series)\n",
    "    print(metric_summary)\n",
    "    print(\"Average model performance metrics:\")\n",
    "    print(metric_summary.mean()) \n",
    "    plot_sequential_performance(metric_summary)\n",
    "    \n",
    "    # average feature importances \n",
    "    average_cfi = pd.DataFrame.from_dict(sequential_results, orient='index')['cfi']\\\n",
    "                              .apply(pd.Series).mean().sort_values(ascending=False)\n",
    "    print(\"Average feature importances:\")\n",
    "    print(average_cfi)     \n",
    "    \n",
    "    return metric_summary, average_cfi, sequential_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sequential_performance(sequential_metrics):\n",
    "    \"\"\" Plots AUC and Accuracy by test year \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(sequential_metrics['ROC AUC'])\n",
    "    plt.plot(sequential_metrics['Accuracy'])\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.title('Accuracy and AUC of Sequential Models')\n",
    "    plt.xlabel('Test Year')\n",
    "    plt.legend(loc='best') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that building the model in a sequential way degraded the performance considerably. Perhaps even more surprisingly, accuracy appears to be worse in later years, which is somewhat counterintuitive given that later models had more data to train on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with same variables as rf_best\n",
    "MODEL = (RandomForestClassifier(n_estimators=100, min_samples_split=2, max_features='sqrt', \n",
    "                                max_depth=60, min_samples_leaf=1, random_state=44))\n",
    "sequential_metrics, average_cfi, sequential_results = fit_sequential_models(\n",
    "    df, MODEL, 'granted', cat_features=FULL_CAT_FEATURES, num_features=FULL_NUM_FEATURES, \n",
    "    impute_methods=FULL_IMPUTE_METHODS, start_year=1994, end_year=2013, print_charts=False, print_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying weight decay to samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason could be that there are periodic shifts in sentiment such that older data might be less informative to newer cases. Let's attempt to weight recent samples more than older samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different values of alpha \n",
    "\n",
    "def tune_weight_decay(df, model, label, cat_features, num_features, impute_methods, start_year, end_year, weight_decay_alphas):\n",
    "    results = [] \n",
    "    for alpha in weight_decay_alphas: \n",
    "        result = {} \n",
    "        metrics_summary, _, _ = fit_sequential_models(\n",
    "            df, model, 'granted', cat_features=FULL_CAT_FEATURES, num_features=FULL_NUM_FEATURES, \n",
    "            impute_methods=FULL_IMPUTE_METHODS, start_year=start_year, end_year=end_year, weight_decay=alpha, \n",
    "            print_metrics=False, print_charts=False)\n",
    "        result['alpha'] = alpha \n",
    "        result['average_accuracy'] = metrics_summary['Accuracy'].mean() \n",
    "        result['average_roc_auc'] = metrics_summary['ROC AUC'].mean() \n",
    "        result['average_log_loss'] = metrics_summary['Log Loss'].mean() \n",
    "        result['average_precision'] = metrics_summary['Precision'].mean() \n",
    "        result['average_recall'] = metrics_summary['Recall'].mean() \n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best alpha\n",
    "MODEL = (RandomForestClassifier(n_estimators=20, min_samples_split=2, max_features='sqrt', \n",
    "                                max_depth=60, min_samples_leaf=1, random_state=44))\n",
    "weight_decay_tuning = tune_weight_decay(df, MODEL, 'granted', cat_features=FULL_CAT_FEATURES, \n",
    "                                        num_features=FULL_NUM_FEATURES, impute_methods=FULL_IMPUTE_METHODS, \n",
    "                                        start_year=1994, end_year=2013, \n",
    "                                        weight_decay_alphas=[.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0])\n",
    "weight_decay_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose alpha=0.9 and re-run \n",
    "MODEL = (RandomForestClassifier(n_estimators=100, min_samples_split=2, max_features='sqrt', \n",
    "                                max_depth=60, min_samples_leaf=1, random_state=44))\n",
    "sequential_metrics_wt, average_cfi_wt, sequential_results_wt = fit_sequential_models(\n",
    "    df, MODEL, 'granted', cat_features=FULL_CAT_FEATURES, num_features=FULL_NUM_FEATURES, \n",
    "    impute_methods=FULL_IMPUTE_METHODS, start_year=1994, end_year=2013, weight_decay=0.9, \n",
    "    print_charts=False, print_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequential models as a tuple to pickle \n",
    "import pickle\n",
    "seq_model_pkl_fname = \"sequential_random_forest_vF.pkl\" \n",
    "#seq_model_pkl_fname = os.path.join(DATAFOLDER, 'sequential_random_forest_vF.pkl') # doesn't work? \n",
    "models_object = tuple([sequential_results_wt[year]['model'] for year in sequential_results_wt.keys()]) \n",
    "with open(seq_model_pkl_fname, \"wb\") as f:\n",
    "    pickle.dump(models_object, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare feature importances   \n",
    "compare_cfi = pd.concat([rf_best_cfi, average_cfi_wt], axis=1)\n",
    "compare_cfi.columns = ['aggregate', 'sequential']\n",
    "compare_cfi['delta'] = compare_cfi['sequential'] - compare_cfi['aggregate']\n",
    "compare_cfi = compare_cfi[(compare_cfi['aggregate'] > 0.01) | (compare_cfi['sequential'] > 0.01)]\n",
    "compare_cfi = compare_cfi.sort_values(by='aggregate', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparison of feature importances \n",
    "\n",
    "ind = np.arange(len(compare_cfi))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8)) \n",
    "agg = ax.barh(ind, compare_cfi['aggregate'], width, color='lightgrey', label='Aggregate Random Forest')\n",
    "seq = ax.barh(ind+width, compare_cfi['sequential'], width, color='blue', label='Sequential Random Forests')\n",
    "\n",
    "ax.set_xlabel('Feature Importances')\n",
    "ax.set_title('Feature Importances of Aggregate vs. Sequential Models')\n",
    "ax.set_yticks(ind + width / 2)\n",
    "ax.set_yticklabels((compare_cfi.index))\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Accuracy and AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get aggregate model test accuracy and roc by year \n",
    "\n",
    "def summarize_perf_by_year(y_test, pred, full_data, start_year, end_year):\n",
    "    \n",
    "    \"\"\" Returns accuracy and roc grouped by year from aggregate model results \"\"\"\n",
    "    \n",
    "    # join predictions back to year \n",
    "    agg_results = pd.DataFrame(y_test)\n",
    "    agg_results.rename(columns={'granted': 'truth'}, inplace=True)\n",
    "    agg_results['pred_proba'] = pred \n",
    "    agg_results['pred'] = (agg_results['pred_proba']>0.5).astype(int)\n",
    "    agg_results = agg_results.merge(df[['datAppealFiled_year']], how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # loop through each year to get accuracy and auc \n",
    "    results = [] \n",
    "    for year in np.arange(start_year, end_year+1, 1): \n",
    "        result = {}\n",
    "        df_year = agg_results[agg_results['datAppealFiled_year'] == year]\n",
    "        result['datAppealFiled_year'] = year\n",
    "        result['accuracy'] = accuracy_score(df_year['truth'], df_year['pred'])\n",
    "        result['roc_auc'] = roc_auc_score(df_year['truth'], df_year['pred_proba'])\n",
    "        results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(results).set_index('datAppealFiled_year')\n",
    "    return results_df[['accuracy', 'roc_auc']]\n",
    "\n",
    "agg_perf_by_year = summarize_perf_by_year(y_test, rf_best_pred, df, 1994, 2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_comparison = agg_perf_by_year.merge(sequential_metrics_wt[['Accuracy', 'ROC AUC']], \n",
    "                                         how='left', left_index=True, right_index=True)\n",
    "perf_comparison.rename(columns={'accuracy': 'Accuracy (Agg)', 'roc_auc': 'ROC AUC (Agg)', \n",
    "                                'Accuracy': 'Accuracy (Seq-W)', 'ROC AUC': 'ROC AUC (Seq-W)'}, inplace=True)\n",
    "perf_comparison = perf_comparison.merge(sequential_metrics[['Accuracy', 'ROC AUC']], \n",
    "                                        how='left', left_index=True, right_index=True)\n",
    "perf_comparison.rename(columns={'Accuracy': 'Accuracy (Seq-UW)', 'ROC AUC': 'ROC AUC (Seq-UW)'}, inplace=True)\n",
    "perf_comparison = perf_comparison[['Accuracy (Agg)', 'Accuracy (Seq-UW)', 'Accuracy (Seq-W)', \n",
    "                                   'ROC AUC (Agg)', 'ROC AUC (Seq-UW)', 'ROC AUC (Seq-W)']]\n",
    "perf_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare aggregate vs. sequential (unweighted)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(perf_comparison['Accuracy (Agg)'], color='grey', label='Aggregate Model')\n",
    "plt.plot(perf_comparison['Accuracy (Seq-UW)'], color='red', label='Sequential Model (UW)')\n",
    "plt.xlabel('datAppealFiled_year')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy: Aggregate vs. Sequential (Unweighted) Models')\n",
    "plt.legend(loc='best'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare sequential (unweighted) vs. sequential (weighted)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(perf_comparison['Accuracy (Seq-UW)'], linestyle='--', color='red', label='Sequential Model (UW)')\n",
    "plt.plot(perf_comparison['Accuracy (Seq-W)'], color='blue', label='Sequential Model (W)')\n",
    "plt.xlabel('datAppealFiled_year')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy: Aggregate vs. Sequential (Weighted) Models')\n",
    "plt.legend(loc='best'); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
